{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Overview** \n",
    "\n",
    "Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. It was founded in 1985 as the result of a merger between Houston Natural Gas and InterNorth, both relatively small regional companies in the U.S. Before its bankruptcy on December 2, 2001, Enron employed approximately 20,000 staff and was one of the world's major electricity, natural gas, communications and pulp and paper companies, with claimed revenues of nearly $101 billion during 2000. Fortune named Enron \"America's Most Innovative Company\" for six consecutive years. At the end of 2001, it was revealed that its reported financial condition was sustained by institutionalized, systematic, and creatively planned accounting fraud, known since as the Enron scandal. Enron has since become a well-known example of willful corporate fraud and corruption. The scandal also brought into question the accounting practices and activities of many corporations in the United States and was a factor in the enactment of the Sarbanes–Oxley Act of 2002. The scandal also affected the greater business world by causing the dissolution of the Arthur Andersen accounting firm. It had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, there was a significant amount of typically confidential information entered into public record, including tens of thousands of emails and detailed financial data for top executives. Enron filed for bankruptcy in the Southern District of New York in late 2001 and selected Weil, Gotshal & Manges as its bankruptcy counsel. It ended its bankruptcy during November 2004, pursuant to a court-approved plan of reorganization, after one of the most complex bankruptcy cases in U.S. history. A new board of directors changed the name of Enron to Enron Creditors Recovery Corp., and emphasized reorganizing and liquidating certain operations and assets of the pre-bankruptcy Enron. On September 7, 2006, Enron sold Prisma Energy International Inc., its last remaining business, to Ashmore Energy International Ltd. (now AEI) ref:https://en.wikipedia.org/wiki/Enron \n",
    "\n",
    "** Goal of Project**\n",
    "\n",
    "The goal is to identify the persons of interest (POI)in the scandal.POIs were ‘individuals who were indicted, reached a settlement, or plea deal with the government, or testified in exchange for prosecution immunity.Using several features in Machine Learning under the four division\n",
    "\n",
    "1. Enron dataset\n",
    "\n",
    "2. Feature processing\n",
    "\n",
    "3. Algorithm \n",
    "\n",
    "4. Validation\n",
    "\n",
    "These features are extracted from financial and email records. Having in mind that those POI will have different patterns from the others Non-POI. These differences should be reflected in financial data, communication patterns, etc. and we can train algorithms to exploit and expose these differences.\n",
    "\n",
    "1. Understanding the Dataset and Question \n",
    "\n",
    "Data exploration (learning, cleaning and preparing the data),\n",
    "feature selecting/engineering (selecting the features which influence mostly on the target, create new features (which explains the target the better than existing) \n",
    "reducing the dimensionality of the data using principal component analysis (PCA)), picking/tuning one of the supervised machine learning algorithm and validating it to get the accurate person of interest identifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration\n",
    "\n",
    "The features in the data fall into three major types, namely financial features, email features and POI labels.\n",
    "There are 146 samples with 20 features and a binary classification (\"poi\"), 2774 data points.\n",
    "Among 146 samples, there are 18 POI and 128 non-POI.\n",
    "Among 2774, there are 1358 (48.96%) data points with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 19 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "dtypes: bool(1), float64(18)\n",
      "memory usage: 21.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import tester\n",
    "\n",
    "features_list = ['poi',\n",
    "                'salary',\n",
    "                'bonus', \n",
    "                'long_term_incentive', \n",
    "                'deferred_income', \n",
    "                'deferral_payments',\n",
    "                'loan_advances', \n",
    "                'other',\n",
    "                'expenses', \n",
    "                'director_fees',\n",
    "                'total_payments',\n",
    "                'exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred',\n",
    "                'total_stock_value',\n",
    "                'to_messages',\n",
    "                'from_messages',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_poi_to_this_person']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# Transform data from dictionary to the Pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(data_dict, orient = 'index')\n",
    "#Order columns in DataFrame, exclude email column\n",
    "df = df[features_list]\n",
    "df = df.replace('NaN', np.nan)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI / non-POI split\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "non-POI    128\n",
       "POI         18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split of POI and non-POI in the dataset\n",
    "poi_non_poi = df.poi.value_counts()\n",
    "poi_non_poi.index=['non-POI', 'POI']\n",
    "print \"POI / non-POI split\"\n",
    "poi_non_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of NaN values in the dataset:  1263\n"
     ]
    }
   ],
   "source": [
    "print \"Amount of NaN values in the dataset: \", df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FindLaw describes NaN values as values of 0 but not the missing value. So NaNs will be replaced with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing 'NaN' in financial features with 0\n",
    "df.ix[:,:15] = df.ix[:,:15].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN values in email features means the information is missing. so spliting the data into 2 classes: POI/non-POI and impute the missing values with median of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "email_features = ['to_messages', 'from_messages', 'from_this_person_to_poi', 'from_poi_to_this_person']\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "\n",
    "#impute missing values of email features \n",
    "df.loc[df[df.poi == 1].index,email_features] = imp.fit_transform(df[email_features][df.poi == 1])\n",
    "df.loc[df[df.poi == 0].index,email_features] = imp.fit_transform(df[email_features][df.poi == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy of the financial data by summing up the payment features and comparing it with the total_payment feature and stock features and comparing with the total_stock_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-102500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3285</td>\n",
       "      <td>102500</td>\n",
       "      <td>3285</td>\n",
       "      <td>0</td>\n",
       "      <td>44093</td>\n",
       "      <td>-44093</td>\n",
       "      <td>944</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137864</td>\n",
       "      <td>0</td>\n",
       "      <td>137864</td>\n",
       "      <td>15456290</td>\n",
       "      <td>2604490</td>\n",
       "      <td>-2604490</td>\n",
       "      <td>15456290</td>\n",
       "      <td>0</td>\n",
       "      <td>523</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    poi  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT     False       0      0                    0                0   \n",
       "BHATNAGAR SANJAY  False       0      0                    0                0   \n",
       "\n",
       "                  deferral_payments  loan_advances   other  expenses  \\\n",
       "BELFER ROBERT               -102500              0       0         0   \n",
       "BHATNAGAR SANJAY                  0              0  137864         0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT              3285          102500                     3285   \n",
       "BHATNAGAR SANJAY         137864        15456290                  2604490   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                    0                      44093   \n",
       "BHATNAGAR SANJAY          -2604490                   15456290   \n",
       "\n",
       "                  total_stock_value  to_messages  from_messages  \\\n",
       "BELFER ROBERT                -44093          944             41   \n",
       "BHATNAGAR SANJAY                  0          523             29   \n",
       "\n",
       "                  from_this_person_to_poi  from_poi_to_this_person  \n",
       "BELFER ROBERT                           6                     26.5  \n",
       "BHATNAGAR SANJAY                        1                      0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data: summing payments features and compare with total_payments\n",
    "payments = ['salary',\n",
    "            'bonus', \n",
    "            'long_term_incentive', \n",
    "            'deferred_income', \n",
    "            'deferral_payments',\n",
    "            'loan_advances', \n",
    "            'other',\n",
    "            'expenses', \n",
    "            'director_fees']\n",
    "df[df[payments].sum(axis='columns') != df.total_payments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-102500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3285</td>\n",
       "      <td>102500</td>\n",
       "      <td>3285</td>\n",
       "      <td>0</td>\n",
       "      <td>44093</td>\n",
       "      <td>-44093</td>\n",
       "      <td>944</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137864</td>\n",
       "      <td>0</td>\n",
       "      <td>137864</td>\n",
       "      <td>15456290</td>\n",
       "      <td>2604490</td>\n",
       "      <td>-2604490</td>\n",
       "      <td>15456290</td>\n",
       "      <td>0</td>\n",
       "      <td>523</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    poi  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT     False       0      0                    0                0   \n",
       "BHATNAGAR SANJAY  False       0      0                    0                0   \n",
       "\n",
       "                  deferral_payments  loan_advances   other  expenses  \\\n",
       "BELFER ROBERT               -102500              0       0         0   \n",
       "BHATNAGAR SANJAY                  0              0  137864         0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT              3285          102500                     3285   \n",
       "BHATNAGAR SANJAY         137864        15456290                  2604490   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                    0                      44093   \n",
       "BHATNAGAR SANJAY          -2604490                   15456290   \n",
       "\n",
       "                  total_stock_value  to_messages  from_messages  \\\n",
       "BELFER ROBERT                -44093          944             41   \n",
       "BHATNAGAR SANJAY                  0          523             29   \n",
       "\n",
       "                  from_this_person_to_poi  from_poi_to_this_person  \n",
       "BELFER ROBERT                           6                     26.5  \n",
       "BHATNAGAR SANJAY                        1                      0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_value = ['exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred']\n",
    "df[df[stock_value].sum(axis='columns') != df.total_stock_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an error in the data entry in the two samples. So  correctting them and checking that everything they are correct (empty DataFrames mean no samples with mistakes in the data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [poi, salary, bonus, long_term_incentive, deferred_income, deferral_payments, loan_advances, other, expenses, director_fees, total_payments, exercised_stock_options, restricted_stock, restricted_stock_deferred, total_stock_value, to_messages, from_messages, from_this_person_to_poi, from_poi_to_this_person]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix['BELFER ROBERT','total_payments'] = 3285\n",
    "df.ix['BELFER ROBERT','deferral_payments'] = 0\n",
    "df.ix['BELFER ROBERT','restricted_stock'] = 44093\n",
    "df.ix['BELFER ROBERT','restricted_stock_deferred'] = -44093\n",
    "df.ix['BELFER ROBERT','total_stock_value'] = 0\n",
    "df.ix['BELFER ROBERT','director_fees'] = 102500\n",
    "df.ix['BELFER ROBERT','deferred_income'] = -102500\n",
    "df.ix['BELFER ROBERT','exercised_stock_options'] = 0\n",
    "df.ix['BELFER ROBERT','expenses'] = 3285\n",
    "df.ix['BELFER ROBERT',]\n",
    "df.ix['BHATNAGAR SANJAY','expenses'] = 137864\n",
    "df.ix['BHATNAGAR SANJAY','total_payments'] = 137864\n",
    "df.ix['BHATNAGAR SANJAY','exercised_stock_options'] = 1.54563e+07\n",
    "df.ix['BHATNAGAR SANJAY','restricted_stock'] = 2.60449e+06\n",
    "df.ix['BHATNAGAR SANJAY','restricted_stock_deferred'] = -2.60449e+06\n",
    "df.ix['BHATNAGAR SANJAY','other'] = 0\n",
    "df.ix['BHATNAGAR SANJAY','director_fees'] = 0\n",
    "df.ix['BHATNAGAR SANJAY','total_stock_value'] = 1.54563e+07\n",
    "df.ix['BHATNAGAR SANJAY']\n",
    "df[df[payments].sum(axis='columns') != df.total_payments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [poi, salary, bonus, long_term_incentive, deferred_income, deferral_payments, loan_advances, other, expenses, director_fees, total_payments, exercised_stock_options, restricted_stock, restricted_stock_deferred, total_stock_value, to_messages, from_messages, from_this_person_to_poi, from_poi_to_this_person]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[stock_value].sum(axis='columns') != df.total_stock_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier Investigation\n",
    "\n",
    "Descriptive statistics determins outliers of the distibution as the values which are higher than Q2 + 1.5IQR or less than Q2 - 1.5IQR, where Q2 median of the distribution, IQR - interquartile range.\n",
    "Here the sum of outlier variables for each person is calculated and sorted in descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of outliers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAY KENNETH L</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREVERT MARK A</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHALLEY LAWRENCE G</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SKILLING JEFFREY K</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAVORATO JOHN J</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCMAHON JEFFREY</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    # of outliers\n",
       "TOTAL                          12\n",
       "LAY KENNETH L                  12\n",
       "FREVERT MARK A                 12\n",
       "WHALLEY LAWRENCE G             11\n",
       "SKILLING JEFFREY K             11\n",
       "LAVORATO JOHN J                 9\n",
       "MCMAHON JEFFREY                 8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = df.quantile(.5) + 1.5 * (df.quantile(.75)-df.quantile(.25))\n",
    "pd.DataFrame((df[1:] > outliers[1:]).sum(axis = 1), columns = ['# of outliers']).\\\n",
    "    sort_values('# of outliers',  ascending = [0]).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data set is really small, a consideration to use 5% of the samples with most number of outlier variables is made:\n",
    "The first value is 'TOTAL' which is the total value of financial payments from the FindLaw data. Total should be excluded because it is not a POI.\n",
    "Kenneth Lay and Jeffrey Skilling are very well known persons from ENRON - they will be kept as they represent anomalies but not the outliers. \n",
    "Mark Frevert and Lawrence Whalley are not so very well known but top managers of the Enron who also represent valuable examples for the model - they will be kept in the data set. \n",
    "John Lavorato is not very well known person as far as I've searched in the internet. I don't think he represents a valid point and exclude him. \n",
    "Jeffrey Mcmahon is the former treasurer who worked before guilty Ben Glisan. I would exclude him from the data set as he worked before the guilty treasurer and might add some confusion to the model. \n",
    "Out of 7 persons 3 are excluded (1 typo 'TOTAL' and 2 persons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "\tAccuracy: 0.33760\tPrecision: 0.14848\tRecall: 0.83800\tF1: 0.25226\tF2: 0.43447\n",
      "\tTotal predictions: 15000\tTrue positives: 1676\tFalse positives: 9612\tFalse negatives:  324\tTrue negatives: 3388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest , f_classif \n",
    "from tester import test_classifier, dump_classifier_and_data \n",
    "scaler = StandardScaler()\n",
    "df_norm = df[features_list]\n",
    "df_norm = scaler.fit_transform(df_norm.ix[:,1:])\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "features_list2 = ['poi']+range(8)\n",
    "\n",
    "my_dataset = pd.DataFrame(SelectKBest(f_classif, k=8).fit_transform(df_norm, df.poi), index = df.index)\n",
    "my_dataset.insert(0, \"poi\", df.poi)\n",
    "my_dataset = my_dataset.to_dict(orient = 'index')  \n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list2)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude 3 outliers from the data set\n",
    "df = df.drop(['TOTAL', 'LAVORATO JOHN J', 'MCMAHON JEFFREY'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimize Feature Selection/Engineering**\n",
    "\n",
    "Using different features and models to standardize features, apply principal component analysis and GaussianNB classifier, also to use decision tree classifier, incl. choosing the features with features importance attribute and tuning the model.\n",
    "Create new features\n",
    "In both strategies creating new features as a fraction of almost all financial variables (f.ex. fractional bonus as fraction of bonus to total_payments, etc.). Logic behind email feature creation was to check the fraction of emails, sent to POI, to all sent emails; emails, received from POI, to all received emails.\n",
    "resulting in using one new feature fraction_to_POI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create additional feature: fraction of person's email to POI to all sent messages\n",
    "df['fraction_to_poi'] = df['from_this_person_to_poi']/df['from_messages']\n",
    "#clean all 'inf' values which we got if the person's from_messages = 0\n",
    "df = df.replace('inf', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree doesn't require  any feature scaling \n",
    "\n",
    "Intelligently select features\n",
    "it is important to sort by null, so we can get all the non-null features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fraction_to_poi', 0.35824390243902443]\n",
      "['expenses', 0.26431889023871075]\n",
      "['to_messages', 0.16306330961503368]\n",
      "['other', 0.084740740740740714]\n",
      "['deferred_income', 0.070617283950617254]\n",
      "['from_poi_to_this_person', 0.059015873015873015]\n"
     ]
    }
   ],
   "source": [
    "#Decision tree using features with non-null importance\n",
    "clf = DecisionTreeClassifier(random_state = 75)\n",
    "clf.fit(df.ix[:,1:], df.ix[:,:1])\n",
    "\n",
    "# show the features with non null importance, sorted and create features_list of features for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for f_i in features_importance:\n",
    "    print f_i\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above fraction_to_poi feature has the highest importance for the model. The number of features used for the model may cause different results. In the algorithm tuning step, features with non-null importance will be chosen so there will be a change in the number.\n",
    "Random state equal to 75 in decision tree and random forest to will be able to represent the results. The exact value was manually chosen for better performance of decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick and Tune an Algorithm**\n",
    "\n",
    "3 machine learning algorithms have been used:\n",
    "\n",
    "- Decision Tree Classifier\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- GaussianNB\n",
    "\n",
    "For decision tree and random forest  features with non-null importance based on clf.features_importances__ is chosen.\n",
    "\n",
    "In the next step the number of features is chnaged from 1 to all in order to achieve the best performance.\n",
    "For the GaussianNB classifier  a number of steps is applied to achieve the result:\n",
    "standardized features; \n",
    "applied SelectKBest function from sklearn to find k best features for the algorithm (resulting in k = 8 which gave me better result for k in a range from 1 to all); \n",
    "PCA is used to decrease the dimensionality of the data (resulting in n_components = 3). \n",
    "Using Decision Tree Classifier showed the best result and was significantly faster than RandomForest whih can be easily tuned.\n",
    "Here are the following results from the algorithms before tuning (using tester.py, provided in advance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>0.90880</td>\n",
       "      <td>0.66255</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.65314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.89780</td>\n",
       "      <td>0.70322</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.51318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.86447</td>\n",
       "      <td>0.49065</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.46003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy  Precision  Recall       F1\n",
       "Decision Tree Classifier   0.90880    0.66255   0.644  0.65314\n",
       "Random Forest              0.89780    0.70322   0.404  0.51318\n",
       "Gaussian Naive Bayes       0.86447    0.49065   0.433  0.46003"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[0.90880, 0.66255, 0.64400, 0.65314],\n",
    "              [0.89780, 0.70322, 0.40400, 0.51318],\n",
    "              [0.86447, 0.49065, 0.43300, 0.46003]],\n",
    "             columns = ['Accuracy','Precision', 'Recall', 'F1'], \n",
    "             index = ['Decision Tree Classifier', 'Random Forest', 'Gaussian Naive Bayes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=75, splitter='best')\n",
      "\tAccuracy: 0.90880\tPrecision: 0.66255\tRecall: 0.64400\tF1: 0.65314\tF2: 0.64763\n",
      "\tTotal predictions: 15000\tTrue positives: 1288\tFalse positives:  656\tFalse negatives:  712\tTrue negatives: 12344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier with standard parametres \n",
    "clf = DecisionTreeClassifier(random_state = 75)\n",
    "my_dataset = df[features_list].to_dict(orient = 'index')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=75, verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.89780\tPrecision: 0.70322\tRecall: 0.40400\tF1: 0.51318\tF2: 0.44158\n",
      "\tTotal predictions: 15000\tTrue positives:  808\tFalse positives:  341\tFalse negatives: 1192\tTrue negatives: 12659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest with standard parameters\n",
    "clf = RandomForestClassifier(random_state = 75)\n",
    "clf.fit(df.ix[:,1:], np.ravel(df.ix[:,:1]))\n",
    "\n",
    "# selecting the features with non null importance, sorting and creating features_list for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')\n",
    "\n",
    "# number of features for best result was found iteratively\n",
    "features_list2 = features_list[:11]\n",
    "my_dataset = df[features_list2].to_dict(orient = 'index')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list2)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "\tAccuracy: 0.86447\tPrecision: 0.49065\tRecall: 0.43300\tF1: 0.46003\tF2: 0.44342\n",
      "\tTotal predictions: 15000\tTrue positives:  866\tFalse positives:  899\tFalse negatives: 1134\tTrue negatives: 12101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB with feature standartization, selection, PCA\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "# data set standartization\n",
    "scaler = StandardScaler()\n",
    "df_norm = df[features_list]\n",
    "df_norm = scaler.fit_transform(df_norm.ix[:,1:])\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "features_list2 = ['poi']+range(3)\n",
    "my_dataset = pd.DataFrame(SelectKBest(f_classif, k=8).fit_transform(df_norm, df.poi), index = df.index)\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=3)\n",
    "my_dataset2 = pd.DataFrame(pca.fit_transform(my_dataset),  index=df.index)\n",
    "my_dataset2.insert(0, \"poi\", df.poi)\n",
    "my_dataset2 = my_dataset2.to_dict(orient = 'index')  \n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset2, features_list2)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune the algorithm**\n",
    "\n",
    "In Machine learning Bias-variance tradeoff is one of the key dilema ,high bias algorithms has no capacity to learn, high variance algorithms react poorly they have no histroy(something in memory). Predictive model is used to arrive at a compromise. The process of changing the parameteres of algorithms is algorithm tuning and it lets us find the golden mean and best result. \n",
    "Algorithm might be tuned manually by iteratively changing the parameteres and tracking the results. Or GridSearchCV might be used which makes this automatically.\n",
    "Here,by tuning the parameteres of the decision tree classifier which is sequentially tuning parameter by parameter and got the best F1 using these parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion = 'entropy', \n",
    "                             min_samples_split = 19,\n",
    "                             random_state = 75,\n",
    "                             min_samples_leaf=6, \n",
    "                             max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=6,\n",
      "            min_samples_split=19, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=75, splitter='best')\n",
      "\tAccuracy: 0.93673\tPrecision: 0.83238\tRecall: 0.65800\tF1: 0.73499\tF2: 0.68678\n",
      "\tTotal predictions: 15000\tTrue positives: 1316\tFalse positives:  265\tFalse negatives:  684\tTrue negatives: 12735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(df.ix[:,1:], df.poi)\n",
    "\n",
    "# show the features with non null importance, sorted and create features_list of features for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')\n",
    "\n",
    "my_dataset = df[features_list].to_dict(orient = 'index')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate and Evaluate**\n",
    "\n",
    "\n",
    "Usage of Evaluation Metrics\n",
    "\n",
    "F1 score is the key to measure the acuracy of hte algorithims in this project. Both Precision and the recall of the test to compute the score.\n",
    "Precision is the ability of the classifier not label as positive sample that is negative.\n",
    "Recall is the ability of the classifier to find all positive samples.\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\n",
    "\n",
    "\n",
    "The tuned decision tree classifier showed precision 0.82238 and recall 0.65800 with the resulting F1 score 0.73499. Explained as 82.24% of the called POI are POI and 65.80% of POI are identified.\n",
    "\n",
    "**Validation Strategy**\n",
    "\n",
    "Validation is a process of evaluating the model performance. Classic mistake is to use small data set for the model training or validate model on the same data set as train it.\n",
    "There are a number of strategies to validate the model. One of them is to split the available data into train and test data another one is to perform a cross validation: process of splitting the data on k beans equal size; run learning experiments; repeat this operation number of times and take the average test result.\n",
    "\n",
    "Algorithm Performance\n",
    " The tester function provided is used ofr validation, it performs stratified shuffle split cross validation approach using StratifiedShuffleSplit function from sklearn.cross_validation library. The results are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>0.93673</td>\n",
       "      <td>0.83238</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.73499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy  Precision  Recall       F1\n",
       "Decision Tree Classifier   0.93673    0.83238   0.658  0.73499"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[0.93673, 0.83238, 0.65800, 0.73499]],\n",
    "             columns = ['Accuracy','Precision', 'Recall', 'F1'], \n",
    "             index = ['Decision Tree Classifier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection (Conclusions)**\n",
    "\n",
    "Approching the project witht he idea that once the right alogorithim is chosen will make a good machine learning.I have realised that having a good(clean) data will help significantly on the algorithim tuning. Here most time was spent on outlier detection and preparing the data\n",
    "I am sure using Random Forest will improve the model significantly\n",
    "\n",
    "Limitations of the study\n",
    "\n",
    "Given that we only had about 145 person in the data, comes with some limitations, since we might skip some other POIs, but getting all the emails and financial inforation of everyone seems a daunting task.The missing email values were given as medians so the modes of the email distribution are switched to medians. The Algorithms were tuned sequentially (swtiching parameters  to achieve better performance. There is a chance that othere parameters in combination might give better model's accuracy).\n",
    "\n",
    "References:\n",
    "Enron data set: https://www.cs.cmu.edu/~./enron/\n",
    "FindLaw financial data: http://www.findlaw.com\n",
    "Visualization of POI: http://www.nytimes.com/packages/html/national/20061023_ENRON_TABLE/index.html\n",
    "Enron on Wikipedia: https://en.wikipedia.org/wiki/Enron\n",
    "F1 score on Wikipedia: https://en.wikipedia.org/wiki/F1_score\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "Udacity Mentor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
